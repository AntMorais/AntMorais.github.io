---
layout: post
title: YOLIME
---

Explainability of YOLOv4 models using LIME 


![an image alt text]({{ site.baseurl }}/images/yolime/yolime.gif "Example explanations")

Explainability methods are used to provide insight into the predictions made by Machine Learning (ML) models, by translating into human-terms the decision that is made. The main reasons for applying explainability to a ML model is to increase its trustworthiness. This is especially important in applications where failures can have serious consequences such as harm to animal life or property/environmental damage.

One example application in this context is the usage of ML models to detect forest fires. There are obvious safety concerns associated with a model failure. For instance, in the case where a forest fire goes unnoticed, this mistake can cause it to spread farther. Because of this, measures should be taken to certify the responsible authorities that the model is performing its job correctly.

Local Interpretable Model-Agnostic Explanations ([LIME](https://github.com/marcotcr/lime)) is a Python library for  explaining  the predictions of  ML  classifiers .   It  is  model-agnostic  anda as such can be applied to any ML classifier that utilises text or images.  Visual explanations for an image from an example [notebook](https://marcotcr.github.io/lime/tutorials/Tutorial%20-%20images.html) can be seen in the following images.


![an image alt text]({{ site.baseurl }}/images/yolime/lime1.png "Original")

![an image alt text]({{ site.baseurl }}/images/yolime/lime2.png "Explanation")

